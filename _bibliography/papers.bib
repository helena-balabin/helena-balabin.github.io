
@inproceedings{araujoHowRelevantSelective2022,
	address = {Online only},
	title = {How {Relevant} is {Selective} {Memory} {Population} in {Lifelong} {Language} {Learning}?},
	url = {https://aclanthology.org/2022.aacl-short.20},
	abstract = {Lifelong language learning seeks to have models continuously learn multiple tasks in a sequential order without suffering from catastrophic forgetting. State-of-the-art approaches rely on sparse experience replay as the primary approach to prevent forgetting. Experience replay usually adopts sampling methods for the memory population; however, the effect of the chosen sampling strategy on model performance has not yet been studied. In this paper, we investigate how relevant the selective memory population is in the lifelong learning process of text classification and question-answering tasks. We found that methods that randomly store a uniform number of samples from the entire data stream lead to high performances, especially for low memory size, which is consistent with computer vision studies.},
	urldate = {2022-11-29},
	booktitle = {Proceedings of the 2nd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 12th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Araujo, Vladimir and Balabin, Helena and Hurtado, Julio and Soto, Alvaro and Moens, Marie-Francine},
	month = nov,
	year = {2022},
	pages = {154--160},
	file = {Full Text PDF:/home/luna.kuleuven.be/u0150403/Zotero/storage/X3ATQZZC/Araujo et al. - 2022 - How Relevant is Selective Memory Population in Lif.pdf:application/pdf},
}

@inproceedings{tammPretrainedSpeechRepresentations2022,
	title = {Pre-trained {Speech} {Representations} as {Feature} {Extractors} for {Speech} {Quality} {Assessment} in {Online} {Conferencing} {Applications}},
	url = {https://www.isca-archive.org/interspeech_2022/tamm22_interspeech.html},
	doi = {10.21437/Interspeech.2022-10147},
	abstract = {Speech quality in online conferencing applications is typically assessed through human judgements in the form of the mean opinion score (MOS) metric. Since such a labor-intensive approach is not feasible for large-scale speech quality assessments in most settings, the focus has shifted towards automated MOS prediction through end-to-end training of deep neural networks (DNN). Instead of training a network from scratch, we propose to leverage the speech representations from the pre-trained wav2vec-based XLS-R model. However, the number of parameters of such a model exceeds task-specific DNNs by several orders of magnitude, which poses a challenge for resulting fine-tuning procedures on smaller datasets. Therefore, we opt to use pre-trained speech representations from XLS-R in a feature extraction rather than a fine-tuning setting, thereby significantly reducing the number of trainable model parameters. We compare our proposed XLS-R-based feature extractor to a Mel-frequency cepstral coefficient (MFCC)-based one, and experiment with various combinations of bidirectional long short term memory (Bi-LSTM) and attention pooling feedforward (AttPoolFF) networks trained on the output of the feature extractors. We demonstrate the increased performance of pre-trained XLS-R embeddings in terms a reduced root mean squared error (RMSE) on the ConferencingSpeech 2022 MOS prediction task.},
	urldate = {2022-10-27},
	booktitle = {Interspeech 2022},
	author = {Tamm, Bastiaan and Balabin, Helena and Vandenberghe, Rik and Van hamme, Hugo},
	month = oct,
	year = {2022},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {4083--4087},
	file = {arXiv Fulltext PDF:/home/luna.kuleuven.be/u0150403/Zotero/storage/ZB3AUKHS/Tamm et al. - 2022 - Pre-trained Speech Representations as Feature Extr.pdf:application/pdf;arXiv.org Snapshot:/home/luna.kuleuven.be/u0150403/Zotero/storage/ZPB8JXR8/2210.html:text/html},
}

@article{madanDeepLearningbasedDetection2022,
	title = {Deep {Learning}-based detection of psychiatric attributes from {German} mental health records},
	volume = {161},
	issn = {1386-5056},
	url = {https://www.sciencedirect.com/science/article/pii/S1386505622000387},
	doi = {10.1016/j.ijmedinf.2022.104724},
	abstract = {Background
Health care records provide large amounts of data with real-world and longitudinal aspects, which is advantageous for predictive analyses and improvements in personalized medicine. Text-based records are a main source of information in mental health. Therefore, application of text mining to the electronic health records – especially mental state examination – is a key approach for detection of psychiatric disease phenotypes that relate to treatment outcomes.
Methods
We focused on the mental state examination (MSE) in the patients’ discharge summaries as the key part of the psychiatric records. We prepared a sample of 150 text documents that we manually annotated for psychiatric attributes and symptoms. These documents were further divided into training and test sets. We designed and implemented a system to detect the psychiatric attributes automatically and linked the pathologically assessed attributes to AMDP terminology. This workflow uses a pre-trained neural network model, which is fine-tuned on the training set, and validated on the independent test set. Furthermore, a traditional NLP and rule-based component linked the recognized mentions to AMDP terminology. In a further step, we applied the system on a larger clinical dataset of 510 patients to extract their symptoms.
Results
The system identified the psychiatric attributes as well as their assessment (normal and pathological) and linked these entities to the AMDP terminology with an F1-score of 86\% and 91\% on an independent test set, respectively.
Conclusion
The development of the current text mining system and the results highlight the feasibility of text mining methods applied to MSE in electronic mental health care reports. Our findings pave the way for the secondary use of routine data in the field of mental health, facilitating further clinical data analyses.},
	language = {en},
	urldate = {2022-10-13},
	journal = {International Journal of Medical Informatics},
	author = {Madan, Sumit and Julius Zimmer, Fabian and Balabin, Helena and Schaaf, Sebastian and Fröhlich, Holger and Fluck, Juliane and Neuner, Irene and Mathiak, Klaus and Hofmann-Apitius, Martin and Sarkheil, Pegah},
	month = may,
	year = {2022},
	keywords = {Clinical Text Mining, Deep Learning, AMDP, Electrical Health Records, Mental State Examination},
	pages = {104724},
	file = {ScienceDirect Full Text PDF:/home/luna.kuleuven.be/u0150403/Zotero/storage/Z3TIGWJL/Madan et al. - 2022 - Deep Learning-based detection of psychiatric attri.pdf:application/pdf;ScienceDirect Snapshot:/home/luna.kuleuven.be/u0150403/Zotero/storage/X3IPNVVE/S1386505622000387.html:text/html},
}

@inproceedings{balabinProtSTonKGsSophisticatedTransformer2022,
	address = {Online},
	title = {{ProtSTonKGs}: {A} {Sophisticated} {Transformer} {Trained} on {Protein} {Sequences}, {Text}, and {Knowledge} {Graphs}},
	abstract = {While most approaches individually exploit unstructured data from the biomedical literature or structured data from biomedical knowledge graphs, their union can better exploit the advantages of such approaches, ultimately improving representations of biology. Using multimodal transformers for such purposes can improve performance on context dependent classification tasks, as demonstrated by our previous model, the Sophisticated Transformer Trained on Biomedical Text and Knowledge Graphs (STonKGs). In this work, we introduce Prot-STonKGs, a transformer aimed at learning all-encompassing representa-
tions of protein-protein interactions. ProtSTonKGs presents an extension to our previous work by adding textual protein descriptions and amino acid sequences (i.e., structural information) to the text- and knowledge
graph-based input sequence used in STonKGs. We benchmark Prot-STonKGs against STonKGs, resulting in improved F 1 scores by up to 0.066 (i.e., from 0.204 to 0.270) in several tasks such as predicting protein
interactions in several contexts. Our work demonstrates how multimodal transformers can be used to integrate heterogeneous sources of information, paving the foundation for future approaches that use multiple modalities for biomedical applications.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Semantic} {Web} {Applications} and {Tools} for {Life} {Sciences}},
	author = {Balabin, Helena and Hoyt, Charles Tapley and Gyori, Benjamin M and Bachman, John,  and Kodamullil, Alpha Tom and Hofmann-Apitius, Martin and Domingo-Fernández, Daniel},
	month = jan,
	year = {2022},
	file = {Balabin et al. - 2022 - ProtSTonKGs A Sophisticated Transformer Trained o.pdf:/home/luna.kuleuven.be/u0150403/Zotero/storage/GUHUU5N8/Balabin et al. - 2022 - ProtSTonKGs A Sophisticated Transformer Trained o.pdf:application/pdf},
}

@article{sargsyanCOVID19Ontology2020,
	title = {The {COVID}-19 {Ontology}},
	volume = {36},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btaa1057},
	doi = {10.1093/bioinformatics/btaa1057},
	abstract = {The COVID-19 pandemic has prompted an impressive, worldwide response by the academic community. In order to support text mining approaches as well as data description, linking and harmonization in the context of COVID-19, we have developed an ontology representing major novel coronavirus (SARS-CoV-2) entities. The ontology has a strong scope on chemical entities suited for drug repurposing, as this is a major target of ongoing COVID-19 therapeutic development.The ontology comprises 2270 classes of concepts and 38 987 axioms (2622 logical axioms and 2434 declaration axioms). It depicts the roles of molecular and cellular entities in virus-host interactions and in the virus life cycle, as well as a wide spectrum of medical and epidemiological concepts linked to COVID-19. The performance of the ontology has been tested on Medline and the COVID-19 corpus provided by the Allen Institute.COVID-19 Ontology is released under a Creative Commons 4.0 License and shared via https://github.com/covid-19-ontology/covid-19. The ontology is also deposited in BioPortal at https://bioportal.bioontology.org/ontologies/COVID-19.Supplementary data are available at Bioinformatics online.},
	number = {24},
	urldate = {2022-01-13},
	journal = {Bioinformatics},
	author = {Sargsyan, Astghik and Kodamullil, Alpha Tom and Baksi, Shounak and Darms, Johannes and Madan, Sumit and Gebel, Stephan and Keminer, Oliver and Jose, Geena Mariya and Balabin, Helena and DeLong, Lauren Nicole and Kohler, Manfred and Jacobs, Marc and Hofmann-Apitius, Martin},
	month = dec,
	year = {2020},
	pages = {5703--5705},
	file = {Full Text PDF:/home/luna.kuleuven.be/u0150403/Zotero/storage/7CET7MXJ/Sargsyan et al. - 2020 - The COVID-19 Ontology.pdf:application/pdf;Snapshot:/home/luna.kuleuven.be/u0150403/Zotero/storage/UI8Z5YSU/6042752.html:text/html},
}

@article{balabinSTonKGsSophisticatedTransformer2022,
	title = {{STonKGs}: a sophisticated transformer trained on biomedical text and knowledge graphs},
	volume = {38},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btac001},
	doi = {10.1093/bioinformatics/btac001},
	abstract = {The majority of biomedical knowledge is stored in structured databases or as unstructured text in scientific publications. This vast amount of information has led to numerous machine learning-based biological applications using either text through natural language processing (NLP) or structured data through knowledge graph embedding models. However, representations based on a single modality are inherently limited.To generate better representations of biological knowledge, we propose STonKGs, a Sophisticated Transformer trained on biomedical text and Knowledge Graphs (KGs). This multimodal Transformer uses combined input sequences of structured information from KGs and unstructured text data from biomedical literature to learn joint representations in a shared embedding space. First, we pre-trained STonKGs on a knowledge base assembled by the Integrated Network and Dynamical Reasoning Assembler consisting of millions of text-triple pairs extracted from biomedical literature by multiple NLP systems. Then, we benchmarked STonKGs against three baseline models trained on either one of the modalities (i.e. text or KG) across eight different classification tasks, each corresponding to a different biological application. Our results demonstrate that STonKGs outperforms both baselines, especially on the more challenging tasks with respect to the number of classes, improving upon the F1-score of the best baseline by up to 0.084 (i.e. from 0.881 to 0.965). Finally, our pre-trained model as well as the model architecture can be adapted to various other transfer learning applications.We make the source code and the Python package of STonKGs available at GitHub (https://github.com/stonkgs/stonkgs) and PyPI (https://pypi.org/project/stonkgs/). The pre-trained STonKGs models and the task-specific classification models are respectively available at https://huggingface.co/stonkgs/stonkgs-150k and https://zenodo.org/communities/stonkgs.Supplementary data are available at Bioinformatics online.},
	number = {6},
	urldate = {2022-08-31},
	journal = {Bioinformatics},
	author = {Balabin, Helena and Hoyt, Charles Tapley and Birkenbihl, Colin and Gyori, Benjamin M and Bachman, John and Kodamullil, Alpha Tom and Plöger, Paul G and Hofmann-Apitius, Martin and Domingo-Fernández, Daniel},
	month = mar,
	year = {2022},
	pages = {1648--1656},
	file = {Full Text PDF:/home/luna.kuleuven.be/u0150403/Zotero/storage/CFYVV34B/Balabin et al. - 2022 - STonKGs A Sophisticated Transformer Trained on Bi.pdf:application/pdf;Full Text PDF:/home/luna.kuleuven.be/u0150403/Zotero/storage/NEJ6U5N8/Balabin et al. - 2021 - STonKGs A Sophisticated Transformer Trained on Bi.pdf:application/pdf;Snapshot:/home/luna.kuleuven.be/u0150403/Zotero/storage/T2Z9FKQM/6497782.html:text/html;Snapshot:/home/luna.kuleuven.be/u0150403/Zotero/storage/V4GEH4M4/2021.08.17.html:text/html},
	selected = {true},
	preview = {stonkgs_logo.png}
}

@inproceedings{balabinInvestigatingNeuralFit2023,
	address = {Kraków, Poland},
	title = {Investigating {Neural} {Fit} {Approaches} for {Sentence} {Embedding} {Model} {Paradigms}},
	url = {https://ebooks-iospress-nl.kuleuven.e-bronnen.be/doi/10.3233/FAIA230267},
	abstract = {In recent years, representations from brain activity patterns and pre-trained language models have been linked to each other based on neural fits to validate hypotheses about language processing. Nonetheless, open questions remain about what intrinsic properties of language processing these neural fits reflect and whether they differ across neural fit approaches, brain networks, and models. In this study, we use parallel sentence and functional magnetic resonance imaging data to perform a comprehensive analysis of four paradigms (masked language modeling, pragmatic coherence, semantic comparison, and contrastive learning) representing linguistic hypotheses about sentence processing. We include three sentence embedding models for each paradigm, resulting in a total of 12 models, and examine differences in their neural fit to four different brain networks using regression-based neural encoding and Representational Similarity Analysis (RSA). Among the different models tested, GPT-2, SkipThoughts, and S-RoBERTa yielded the strongest correlations with language network patterns, whereas contrastive learning-based models resulted in overall low neural fits. Our findings demonstrate that neural fits vary across brain networks and models representing the same linguistic hypothesis (e.g., GPT-2 and GPT-3). More importantly, we show the need for both neural encoding and RSA as complementary methods to provide full understanding of neural fits. All code used in the analysis is publicly available: https://github.com/lcn-kul/sentencefmricomparison.},
	urldate = {2023-10-02},
	booktitle = {Proceedings of the 26th {European} {Conference} on {Artificial} {Intelligence}},
	publisher = {IOS Press},
	author = {Balabin, Helena and Liuzzi, Antonietta Gabriella and Sun, Jingyuan and Dupont, Patrick and Vanderberghe, Rik and Moens, Marie-Francine},
	month = oct,
	year = {2023},
	doi = {10.3233/FAIA230267},
	pages = {165--173},
	file = {Full Text PDF:/home/luna.kuleuven.be/u0150403/Zotero/storage/CHLXYCND/Balabin et al. - 2023 - Investigating Neural Fit Approaches for Sentence E.pdf:application/pdf},
	selected = {true},
	preview = {ECAI.png}
}

@inproceedings{lirias4132564,
	address = {Vancouver, Canada},
	title = {A comparative analysis of language models for the classification of alzheimer’s disease based on connected speech},
	abstract = {Early diagnosis of neurological disorders is a fast developing field of applied artificial intelligence (AI). In this context, AI- based language models have been increasingly used to distinguish cognitively healthy individuals from those affected by Alzheimer’s disease (AD) based on their connected speech. Yet, it remains unknown how the adaptation of the language models to the language and domain of the connected speech samples impacts the classification results. Here, we construct several classification tasks from Dutch Flemish samples of connected speech from a cohort of 74 healthy controls and 20 subjects affected by AD. First, we compare the classification performance of Dutch and multilingual models as well as models that incorporate long-range context. Additionally, we examine how varying amounts of fine-tuning data from a separate corpus of speech samples affect domain adaptation. We demonstrate that increasing fine-tuning data leads to increased domain adaptation, but it does not necessarily translate into improved classification performance. Furthermore, our findings support the use of language-specific models over multilingual ones, even for multilingual models that were trained to incorporate wider context.},
	booktitle = {Proceedings of the {First} {Workshop} on {Artificial} {Intelligence} for {Brain} {Encoding} and {Decoding}},
	author = {Balabin, Helena and Spruyt, Laure and Eycken, Ella and Kabouche, Ines and Tamm, Bastiaan and Schaeverbeke, Jolien and Dupont, Patrick and Moens, Marie-Francine and Vandenberghe, Rik},
	month = feb,
	year = {2024},
	keywords = {C14/21/109\#56286979},
	file = {Balabin et al. - 2024 - A comparative analysis of language models for the .pdf:/home/luna.kuleuven.be/u0150403/Zotero/storage/FDYZVS4K/Balabin et al. - 2024 - A comparative analysis of language models for the .pdf:application/pdf},
}

@article{wegnerSemanticHarmonizationAlzheimer2024,
	title = {Semantic harmonization of {Alzheimer}'s disease datasets using {AD}-{Mapper}},
	urldate = {2024-04-23},
	journal = {[Accepted] Journal of Alzheimer's Disease},
	author = {Wegner, Philipp and Balabin, Helena and Ay, Mehmet Can and Bauermeister, Sarah and Killin, Lewis and Gallacher, John and Hofmann-Apitius, Martin and Salimi, Yasamin},
	month = apr,
	year = {2024},
	file = {Available Version (via Google Scholar):/home/luna.kuleuven.be/u0150403/Zotero/storage/2QYRXPJ8/Wegner et al. - 2023 - Semantic harmonization of Alzheimer's disease data.pdf:application/pdf},
}
